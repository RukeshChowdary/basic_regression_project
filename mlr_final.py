# -*- coding: utf-8 -*-
"""mlr_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k_OIVbIB23o5YBpEE2WdadDhQtD0OlLA
"""

#import the data set
import pandas as pd
df = pd.read_csv("/content/ToyotaCorolla - MLR.csv")
df.info()

#seperate y variable
y = df["Price"]
y.head()

# seperating x variable
x = df.iloc[:,1:]
x.head()

#standaradization for x cont
x_cont = x.drop(["Fuel_Type"], axis = 1)
x_cont.head()
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
ss_x = sc.fit_transform(x_cont)
ss_x = pd.DataFrame(ss_x, columns = x_cont.columns)
ss_x.head()

# label encoding for x cat
x_cat = x['Fuel_Type']
x_cat
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
x_cat= le.fit_transform(x_cat)
x_cat = pd.DataFrame(x_cat, columns = ["Fuel_Type"])
x_cat.head()

# joining standardized data and label encoded data
ss_x = ss_x.join(x_cat)
ss_x.head()

#linear regression model fitting
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(ss_x,y,test_size=0.2)
model = LinearRegression()
model.fit(x_train,y_train)
y_predit_train = model.predict(x_train)
y_predit_test = model.predict(x_test)
from sklearn.metrics import r2_score
training_accuracy = r2_score(y_train,y_predit_train)
testing_accuracy = r2_score(y_test,y_predit_test)
print("Training Accuracy : ",training_accuracy)
print("Testing Accuracy : ",testing_accuracy)

#cross validation
from sklearn.model_selection import ShuffleSplit, cross_val_score
cv = ShuffleSplit(n_splits = 100, test_size = 0.2, random_state = 42)
cv_scores = cross_val_score(model,ss_x,y,scoring="r2",cv=cv)
cv_scores = cv_scores.mean()
print("Cross Validation Score : ",cv_scores)

#using lassocv
from sklearn.linear_model import LassoCV
lasso_model= LassoCV()
lasso_model = LassoCV(cv = 10, max_iter = 200000, random_state = 42)
lasso_model.fit(x_train,y_train)

lasso_model.alpha_

y_train_pred = lasso_model.predict(x_train)
y_test_pred = lasso_model.predict(x_test)
print("Training Accuracy : ",r2_score(y_train,y_train_pred))
print("Testing Accuracy : ",r2_score(y_test,y_test_pred))

lasso_model.coef_

ss_x.head()

df_lasso_removedcoeff = ss_x[["Age_08_04","KM","HP","Gears","Weight"]]
df_lasso_removedcoeff.head()

#refitting lassocv after removing coeff
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(df_lasso_removedcoeff,y,test_size=0.2)
from sklearn.linear_model import LassoCV
lasso_model2= LassoCV()
lasso_model2 = LassoCV(cv = 10, max_iter = 200000, random_state = 42)
lasso_model2.fit(x_train,y_train)

lasso_model2.alpha_

y_train_pred = lasso_model2.predict(x_train)
y_test_pred = lasso_model2.predict(x_test)
print("Training Accuracy : ",r2_score(y_train,y_train_pred))
print("Testing Accuracy : ",r2_score(y_test,y_test_pred))

#ridgecv
from sklearn.linear_model import RidgeCV
ridge_model = RidgeCV()
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(ss_x,y,test_size=0.2,random_state=9)
import numpy as np
alphas = np.random.uniform(low = 0, high = 100, size = (50,))
ridge_model = RidgeCV(alphas = alphas, cv = 10)
ridge_model.fit(x_train,y_train)
y_train_pred = ridge_model.predict(x_train)
y_test_pred = ridge_model.predict(x_test)
print("Training Accuracy : ",r2_score(y_train,y_train_pred))
print("Testing Accuracy : ",r2_score(y_test,y_test_pred))

ridge_model.alpha_

ridge_model.coef_

ss_x_ridge =  ss_x.drop(["Cylinders"], axis = 1)
ss_x_ridge.head()

#reffiting model with ridgecv, removed coefficient which are near to zero
from sklearn.linear_model import RidgeCV
ridge_model2 = RidgeCV()
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(ss_x_ridge,y,test_size=0.2, random_state=9)
import numpy as np
alphas = np.random.uniform(low = 0, high = 100, size = (50,))
ridge_model2 = RidgeCV(alphas = alphas, cv = 10)
ridge_model2.fit(x_train,y_train)
y_train_pred = ridge_model2.predict(x_train)
y_test_pred = ridge_model2.predict(x_test)
print("Training Accuracy : ",r2_score(y_train,y_train_pred))
print("Testing Accuracy : ",r2_score(y_test,y_test_pred))

"""After fitting the model with lassocv the traing accuracy and test accuracy are [Training Accuracy :  0.8606541546639459
Testing Accuracy :  0.8703946971771988],
and after fitting the model with ridgecv the training accuracy and test accuracy are
[Training Accuracy :  0.8601575237416416
Testing Accuracy :  0.8623644196277453 ]
Hence, we can conclude that the lassocv model is the best model for the given data set, even there so no much difference in the accuracy scores of lassocv and ridgecv, the lassocv gives us the best result for the given data set.

What is Normalization & Standardization and how is it helpful?
Ans : Normalization is the process of scaling data to a fixed range, typically [0, 1], to ensure all features contribute equally.
Standardization is the process of transforming data to have a mean of 0 and a standard deviation of 1, making features comparable in scale.
The standardized values will not have any unit and these values are between [-3 to +3].
How it is Helpful ?
Normalization and standardization help improve the performance and stability of machine learning models by ensuring that all features are on a similar scale. Without scaling, features with larger values can dominate the model and lead to biased or suboptimal results. These techniques make the model training process faster, more accurate, and help regularization methods (like Ridge or Lasso) work effectively.

What techniques can be used to address multicollinearity in multiple linear regression?
ANS : The techniques used to adress multicollinearity issues are correlation, if any two x variables have high correlation then it is considered as multicollinearity and only one x variable among those two is considered for model fitting.
1. we can use pca(principle component analysis)
2. If we avoid variable selction using correlation, we can utilize ridecv and lassocv.
"""